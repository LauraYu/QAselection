{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "!pip install -U portalocker>=2.0.0\n",
        "!pip install datasets torch transformers\n",
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -qq cornell_movie_dialogs_corpus.zip\n",
        "!rm cornell_movie_dialogs_corpus.zip\n",
        "!mkdir datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n",
        "!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets"
      ],
      "metadata": {
        "id": "XoVhmSfatd-j",
        "outputId": "445cf1e7-043e-40d0-e84d-87273169c2d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "--2024-11-02 19:09:30--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.53\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip [following]\n",
            "--2024-11-02 19:09:30--  https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  13.5MB/s    in 0.7s    \n",
            "\n",
            "2024-11-02 19:09:31 (13.5 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "eNjuOl-Vt6m_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing\n",
        "max_len = 25\n",
        "\n",
        "def remove_punc(string):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in string:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char  # space is also a character\n",
        "    return no_punct.lower()\n",
        "\n",
        "corpus_movie_conv = './datasets/movie_conversations.txt'\n",
        "corpus_movie_lines = './datasets/movie_lines.txt'\n",
        "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
        "    conv = c.readlines()\n",
        "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
        "    lines = l.readlines()\n",
        "\n",
        "# extract text\n",
        "lines_dic = {}\n",
        "for line in lines:\n",
        "    objects = line.split(\" +++$+++ \")\n",
        "    lines_dic[objects[0]] = objects[-1]\n",
        "\n",
        "# generate question answer pairs\n",
        "pairs = []\n",
        "for con in conv:\n",
        "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        qa_pairs = []\n",
        "\n",
        "        if i == len(ids) - 1:\n",
        "            break\n",
        "\n",
        "        first = remove_punc(lines_dic[ids[i]].strip())\n",
        "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
        "        qa_pairs.append(first.split()[:max_len])\n",
        "        qa_pairs.append(second.split()[:max_len])\n",
        "        pairs.append(qa_pairs)\n"
      ],
      "metadata": {
        "id": "M_zM0Gcnt-ij"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pairs))\n",
        "print(pairs[0][0])\n",
        "print(pairs[0][1])"
      ],
      "metadata": {
        "id": "gcwmWnGAuIWH",
        "outputId": "6d8039f0-7e6c-42ee-f2e5-6da5cb5473de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221616\n",
            "['can', 'we', 'make', 'this', 'quick', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break', 'up', 'on', 'the', 'quad', 'again']\n",
            "['well', 'i', 'thought', 'wed', 'start', 'with', 'pronunciation', 'if', 'thats', 'okay', 'with', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_word_freq = 5\n",
        "word_freq = Counter()\n",
        "for pair in pairs:\n",
        "  word_freq.update(pair[0])\n",
        "  word_freq.update(pair[1])\n",
        "\n",
        "words = [word for word, c in word_freq.items() if c > min_word_freq]\n",
        "word_map = {word: i+1 for i, word in enumerate(words)}\n",
        "word_map['<unk>'] = len(word_map) + 1\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0\n",
        "\n",
        "print(\"Total words are: {}\".format(len(word_map)))\n",
        "\n",
        "def encode_question(words, word_map):\n",
        "  enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
        "  return enc_c\n",
        "\n",
        "def encode_reply(words, word_map):\n",
        "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
        "        [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
        "    return enc_c\n",
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "  ques = encode_question(pair[0], word_map)\n",
        "  ans = encode_reply(pair[1], word_map)\n",
        "  pairs_encoded.append([ques, ans])\n",
        "\n",
        "print(pairs_encoded[0][0])\n",
        "print(pairs_encoded[0][1])"
      ],
      "metadata": {
        "id": "bUxsfSy6uSGS",
        "outputId": "6b53ea7c-db3d-47d2-b00e-d454822d5e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words are: 18243\n",
            "[1, 2, 3, 4, 5, 18240, 18240, 6, 7, 8, 9, 10, 11, 12, 18240, 13, 14, 15, 16, 17, 18240, 18, 0, 0, 0]\n",
            "[18241, 19, 20, 21, 22, 23, 24, 18240, 25, 26, 27, 24, 28, 18242, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "  attn_size = (1, size, size)\n",
        "  mask = torch.triu(torch.ones(attn_size), diagonal=1).type(torch.uint16)\n",
        "  return mask == 0\n",
        "\n",
        "def create_mask(src, tgt, pad):\n",
        "  # batch x seq_len\n",
        "  size = tgt.size()[1]\n",
        "  # batch x 1 x seq_len\n",
        "  src_mask = (src != pad).unsqueeze(-2)\n",
        "  tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "  tgt_mask = tgt_mask & subsequent_mask(size)\n",
        "  return src_mask, tgt_mask\n",
        "\n",
        "\n",
        "src = torch.LongTensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 0]\n",
        "    ])\n",
        "tgt = torch.LongTensor([\n",
        "    [1, 2, 3, 4],\n",
        "    [1, 2, 0, 0]\n",
        "])\n",
        "pad = 0\n",
        "src_mask, tgt_mask = create_mask(src, tgt, pad)\n",
        "print(src_mask)\n",
        "print(tgt_mask)"
      ],
      "metadata": {
        "id": "inp8RtRaIW9y",
        "outputId": "8f5eda22-f1ce-4e12-f0c2-8c374b29ffe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ True,  True,  True]],\n",
            "\n",
            "        [[ True,  True, False]]])\n",
            "tensor([[[ True, False, False, False],\n",
            "         [ True,  True, False, False],\n",
            "         [ True,  True,  True, False],\n",
            "         [ True,  True,  True,  True]],\n",
            "\n",
            "        [[ True, False, False, False],\n",
            "         [ True,  True, False, False],\n",
            "         [ True,  True, False, False],\n",
            "         [ True,  True, False, False]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout, max_len=5000):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass"
      ],
      "metadata": {
        "id": "kZnN-UsQKEAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "  pass\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self, h, d_model, dropout=0.1):\n",
        "    pass\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    pass"
      ],
      "metadata": {
        "id": "qeKsX4QJKthn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self, siez, dropout):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    pass"
      ],
      "metadata": {
        "id": "ukJzXICJLNz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, ffn, dropout):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    pass\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, cross_attn, fnn, dropout):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    pass\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    pass\n",
        "\n",
        "  def forwward(self, x, memory, src_mask, tgt_mask):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_embed, tgt_embed, encoder, decoder, generator):\n",
        "    pass\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    pass\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    pass\n",
        "\n",
        "  def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "    pass\n",
        "\n",
        "def create_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "  pass"
      ],
      "metadata": {
        "id": "pdY_YAz_LzQm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}